We developed a baseline IR system for patent prior art search on the top of
the Lucene search engine\footnote{\texttt{http://lucene.apache.org/}}, which processes queries using both BM25~\citep{Robertson1993} and LM (Dirichlet
smoothing, and Jelinek-Mercer smoothing)~\citep{zhai2004study} scoring functions. %We consider 
%using BM25F~\cite{robertson2004simple} but had no opportunity to learn appropriate field weights.
We used Lucene to index the English subset of CLEF-IP 2010 dataset\footnote{\texttt{http://www.ifs.tuwien.ac.at/\textasciitilde{}clef-ip/}} 
(We will describe CLEF-IP in Section~\ref{sec:DataCollection}) that contains 2.6 million patent documents and $1303$ topics (queries) for the English test set.
We used the default Lucene settings with the Porter stemming algorithm \cite{Porter1980} and English stop-word removal. 
We also removed patent-specific stop-words as described in \cite{magdy2012toward}.
In
our implementation, each section of a patent (title, abstract, claims,
and description) is indexed in a separate field. However, when a query 
is processed, all indexed fields are targeted with an equal weight, since this generally
offers best retrieval performance. We also used the International
Patent Classification (IPC) codes assigned to the topics to filter
the search results by constraining them to have common IPC codes with
the patent topic as suggested in previous works~\citep{lopez2010patatras}.
Although this IPC code filter may prevent retrieval of relevant patents 
--- as it will be explained in Section~\ref{sec:ClassificationCodeMismatch} --- we
keep it for the following reasons: (i) more than 80\%
of the patent queries share an IPC code with their associated relevant
patents, and (ii) it makes the retrieval process much faster. The accuracy of the results is evaluated 
using three popular metrics --- Mean Average Precision (MAP), Average Recall, and Patent Retrieval Evaluation 
Score~\citep{magdy2012toward} (PRES) --- on the top-100 results for each query, assuming that patent examiners 
are willing to assess the top 100 patents \citep{joho2010survey}. 

We achieved the best performance while querying with the description
section as in previous work \citep{xue2009transforming} and using
either the LM or the BM25 scoring function. 
We call this initial 
query the Patent Query and use it as our main baseline.
\begin{savenotes}
\begin{table*}[t!]
  \begin{center}
  \caption{Comparing performance metrics for different IR models and query formulation.}
  \input table/IRmodels_Sections.tex 
  \label{tab:IRmodels_Sections}
  \end{center}  
\end{table*}
\end{savenotes}
%\FloatBarrier 
%\noindent

Table~\ref{tab:IRmodels_Sections} compares the system performance for different IR models 
(BM25, TF-IDF, LM with Dirichlet, and Jelinek-Mercer smoothing --- Section~\ref{subsub:retmodels}) with Lucene default settings and different sections of the patent query.
It can be seen that the results for BM25 and LM with Dirichlet (LMDIR) are very similar, therefore in the this thesis, we report our results based on LM.    


In addition, we compare our results to \textit{PATATRAS}, a highly engineered system developed by~\cite{lopez2010experiments}, which achieved the best performance in the CLEF-IP 2010 competition. This system uses multiple retrieval models (especially Kullback-Leibler divergence~\citep{Baeza-Yates2011} and Okapi BM25) and exploits patent meta-data and citation structures. While our evaluation excludes 22 of the 1303 topics for which no relevant English documents were available, the difference in the MAP score between our evaluation and the full 1303 topic evaluation of PATATRAS is negligible. We exclude 22 queries because the focus of our research has been on term analysis and errors related to term matching process of ranking functions. Therefore, we eliminated data curation errors and IPC filter errors --- as they will be described in Section~\ref{sec:DataCurationErrors} and Section~\ref{sec:ClassificationCodeMismatch} --- to increase the accuracy of our data analysis results. 
%did not include errors related to non-English patents in our evaluation and we mainly focus on errors related to term matching for English patents.  
%However, the MAP we provide is not directly comparable since we excluded 22 topics from our evaluation because their relevant patents were not in English or had no IPC code matched with the topic. We pruned these topics out because we were only interested in analysing errors related to term matching. Removing 22 topics caused only 0.04 improvement in our baseline system, which is negligible. Hence, we use Top CLEF-IP 2010 results for a rough comprasion."}
% 

% TODO: Don't exclude 22 topics!
