\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces The main differences between patent prior art search and an standard web search are: (i) queries are reference patent applications, and (ii) patent prior art search is a recall-oriented task.\relax }}{1}{figure.caption.7}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces A sample patent XML file.\relax }}{6}{figure.caption.8}
\contentsline {figure}{\numberline {2.2}{\ignorespaces An example illustrating the main components of an International Patent Classification code.\relax }}{8}{figure.caption.14}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Simple illustration of the process in a general IR system.\relax }}{9}{figure.caption.16}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Rocchio algorithm for relevance feedback. Some documents have been labelled as relevant and non-relevant and the initial query vector is moved in response to this feedback\nobreakspace {}\citep {manning2008introduction}.\relax }}{14}{figure.caption.22}
\contentsline {figure}{\numberline {2.5}{\ignorespaces PRES curve is bounded between the best case and the new defined worst case\nobreakspace {}\citep {magdy2010pres}.\relax }}{25}{figure.caption.35}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces (a) Percentage of English, German, and French patents in CLEF-IP 2010 collection. (b) Completeness of the presence of English text in the CLEF-IP 2010 patent collection.\nobreakspace {}\citep {magdy2012toward}.\relax }}{30}{figure.caption.37}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {{}}}}{30}{figure.caption.37}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{30}{figure.caption.37}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Average percentage of errors due to missing description, language. Overall, $37\%$ of errors are because of data curation while $63\%$ of English complete patent documents cannot be retrieved. Increasing $k$ from $100$ to $1,000$ reduces the errors of the yellow area, but the value of $42\%$ is still notable.\relax }}{31}{figure.caption.38}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Cut-off rank ($k$) $= 100$}}}{31}{figure.caption.38}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Cut-off rank ($k$) $= 1,000$}}}{31}{figure.caption.38}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Classification code overlap between the query and non-relevant retrieved patents (False Negative (FN) patents).\relax }}{32}{figure.caption.40}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Not-retrieved relevant patents (False Negative (FN) patents)}}}{32}{figure.caption.40}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Retrieved relevant patents (True Positive (TP) patents)}}}{32}{figure.caption.40}
\contentsline {figure}{\numberline {3.4}{\ignorespaces The distribution of the number of patents that should be ranked for each query over all test topics ($1,303$), after applying the IPC filter (filter type I). On average, the matching process for each query is done over $ 36,254 $ documents instead of the whole collection (2.6 million documents), which dramatically reduces the computational time.\relax }}{33}{figure.caption.41}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Applying first two IPC code components (Section and Class) for filtering\relax }}{34}{figure.caption.43}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {The portion of patents in the collection which are matched with the query IPC code. }}}{34}{figure.caption.43}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {The distribution of the number of patents should be ranked for each query over all test queries ($1,303$). In average, the matching process for each query is done over $ 99,754 $ documents instead of the whole collection (2.6 million documents), which dramatically reduce the computational time.}}}{34}{figure.caption.43}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Applying the first IPC code component for filtering (Section)\relax }}{35}{figure.caption.45}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {The portion of patents in the collection which are matched with the query IPC code. Filter: The first two components }}}{35}{figure.caption.45}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {The distribution of the number of patents should be processed for each query after applying the IPC filter. In average, the matching process for each query is done over $ 415,828 $ documents instead of the whole collection (2.6 million documents). This number is much higher than using more restricted filters, so it is not computationally efficient.}}}{35}{figure.caption.45}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces The distribution of term overlap between the query and documents over 1,303 test queries.\relax }}{38}{figure.caption.47}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Query and TP patents.}}}{38}{figure.caption.47}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Query and FP patents.}}}{38}{figure.caption.47}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Query and FN patents.}}}{38}{figure.caption.47}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Scatter plot of Recall versus the existence of Useful Terms in query.\relax }}{39}{figure.caption.48}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Useful Terms: $ \{t|RF(t, Q)>0\} $}}}{39}{figure.caption.48}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Useful Terms: $ \{t|RF(t, Q)>RF(t_{+median})\} $}}}{39}{figure.caption.48}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Useful Terms: $ \{t|RF(t, Q)>1 \}$}}}{39}{figure.caption.48}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Useful Terms: top 100 high-scored terms}}}{39}{figure.caption.48}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {Useful Terms: $ \{t|RF(t,Q)>5 \}$}}}{39}{figure.caption.48}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {Useful Terms: $ \{t|RF(t, Q)>10\} $}}}{39}{figure.caption.48}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Scatter plot of Average Precision versus the existence of Useful Terms in query.\relax }}{40}{figure.caption.49}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Useful terms: $ \{t|RF(t, Q)>0\} $}}}{40}{figure.caption.49}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Useful terms: $ \{t|RF(t, Q)>RF(t_{+median}, Q)\} $}}}{40}{figure.caption.49}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Useful terms: $ \{t|RF(t, Q)>1 \}$}}}{40}{figure.caption.49}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Useful terms: top 100 high-scored terms}}}{40}{figure.caption.49}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {Useful terms: $ \{t|RF(t,Q)>5 \}$}}}{40}{figure.caption.49}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {Useful terms: $ \{t|RF(t, Q)>10\} $}}}{40}{figure.caption.49}
\contentsline {figure}{\numberline {4.4}{\ignorespaces The distribution of the term overlap between the query and Useful Terms/Noisy Terms in TPs and FPs. Relevant patents have higher term overlap with Useful Terms while irrelevant patents have higher term overlap with Noisy Terms.\relax }}{41}{figure.caption.50}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {TPs}}}{41}{figure.caption.50}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {FPs}}}{41}{figure.caption.50}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Oracular Query performance versus various values of the threshold $\tau $ and query size\relax }}{43}{figure.caption.54}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Oracular Query performance versus the threshold $\tau $.}}}{43}{figure.caption.54}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Oracular Query performance versus the query size.}}}{43}{figure.caption.54}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Comparing the performance of Oracular Query and Oracular Patent Query for various values of the threshold $\tau $\relax }}{44}{figure.caption.55}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Mean Average Precision $\tau $.}}}{44}{figure.caption.55}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Average Recall}}}{44}{figure.caption.55}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Comparing system performance for three different query reduction approaches and their changes with a threshold $\tau $.\relax }}{45}{figure.caption.56}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Mean Average Precision.}}}{45}{figure.caption.56}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Average Recall.}}}{45}{figure.caption.56}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Anecdotal example for simple query reduction approaches. Blue points are all terms in a vocabulary set made of top-100 retrieved documents and red points are terms in the Patent Query.\relax }}{47}{figure.caption.60}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Query terms ($t \in Query$) versus Document Frequent terms.}}}{47}{figure.caption.60}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Query terms ($t \in Query \mskip \thickmuskip \bigwedge \mskip \thickmuskip QTF(t)>5$) versus Document Frequent terms}}}{47}{figure.caption.60}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Query reduction using PRF for various value of the threshold $\tau $.\relax }}{48}{figure.caption.61}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Comparing $\mathit {RF}$ score of top Relevance Feedback terms and Pseudo Relevance Feedback terms for different values of the threshold $\tau $.\relax }}{48}{figure.caption.62}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$\tau =0$}}}{48}{figure.caption.62}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$\tau =1$}}}{48}{figure.caption.62}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {$\tau =10$}}}{48}{figure.caption.62}
\contentsline {figure}{\numberline {4.11}{\ignorespaces Four query reduction approaches on a sample query. Top terms retained by each method are shown. Numerical oracular scores $\mathit {RF}(t,Q)$ are provided indicating whether the term was useful (blue/positive) or noisy (red/negative).\relax }}{50}{figure.caption.63}
\contentsline {figure}{\numberline {4.12}{\ignorespaces The distribution of the first relevant document rank over test queries.\relax }}{51}{figure.caption.65}
\addvspace {10\p@ }
